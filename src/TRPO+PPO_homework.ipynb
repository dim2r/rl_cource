{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TRPO+PPO_homework.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdiMryk7N1gU"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this homework you will work with TRPO and PPO algorithms. You need to adjust parameters and  fill placeholders with your own code. As a result you'll get a trained virtual robot. \n",
        "\n",
        "Code execution types : \n",
        "\n",
        "1) you may use url in order to access this notebook\n",
        "\n",
        "2) or you may upload .ipynb file into google collab and execute it\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/dim2r/rl_cource/master/img/ant.jpg)\n",
        "\n",
        "Train this robot to walk!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_W6wLBkniRY"
      },
      "source": [
        "## Global parametrs.Define environment name and training time [[*EDIT HERE*]]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FLQELy80oyg"
      },
      "source": [
        "#Pybullet env name\n",
        "config_env_name = 'AntPyBulletEnv-v0' \n",
        "\n",
        "\n",
        "# Time limit in minutes for training of one agent. \n",
        "# So total time will be as long as two this periods  \n",
        "# 30 min - poor result\n",
        "# 120 min - fair result\n",
        "config_train_time_minutes = 120 \n",
        "\n",
        "\n",
        "# Choose algorithm to use\n",
        "config_use_trpo = True #optional\n",
        "config_use_ppo = True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "config_save_dir = 'saved-sessions'\n",
        "\n",
        "import sys\n",
        "#Configure PATH to your pg repository.\n",
        "#You may mount google drive using the following two commands\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# example: config_pg_folder_path = '/content/drive/MyDrive/PG/'\n",
        "config_pg_folder_path = './'\n",
        "sys.path.append(config_pg_folder_path)\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  config_i_am_in_collab = True\n",
        "  print('Running in google collab')\n",
        "else:\n",
        "  config_i_am_in_collab = False\n",
        "  print('I am not in google collab')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XljqTrGJamUx"
      },
      "source": [
        "## Install modules and initialize environment. Run this  before you start. It takes about 5 minutes. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pDk_pxWrbkK"
      },
      "source": [
        "# Step 1. Install required packages\n",
        "\n",
        " \n",
        "import os\n",
        "if config_i_am_in_collab:\n",
        "    print('downloading modules')\n",
        "    os.system(\"curl -o layers.py https://raw.githubusercontent.com/dim2r/rl_cource/master/src/layers.py\")\n",
        "    os.system(\"curl -o pg_utils.py https://raw.githubusercontent.com/dim2r/rl_cource/master/src/pg_utils.py\")\n",
        "    os.system(\"curl -o trpo.py https://raw.githubusercontent.com/dim2r/rl_cource/master/src/trpo.py\")\n",
        "\n",
        "\n",
        "!pip install torch torchvision gym numpngw numpy pyglet PyYAML jdc matplotlib  \n",
        "!pip install pybullet\n",
        "!pip install git+https://github.com/benelot/pybullet-gym\n",
        "\n",
        "    \n",
        "# Step 2. Fix installation\n",
        "import shutil \n",
        "import pybulletgym\n",
        "libdir = os.path.dirname(os.path.dirname(pybulletgym.__file__))\n",
        "def recursive_copy(src, dest):\n",
        "    if os.path.isdir(src):\n",
        "        if not os.path.isdir(dest):\n",
        "            os.makedirs(dest)\n",
        "        files = os.listdir(src)\n",
        "        for f in files:\n",
        "            recursive_copy(os.path.join(src, f), \n",
        "                           os.path.join(dest, f)\n",
        "                          )\n",
        "    else:\n",
        "        #print(src+\"->>\"+dest)\n",
        "        shutil.copyfile(src, dest)\n",
        "\n",
        "recursive_copy(f\"{libdir}/pybullet_data/\",f\"{libdir}/pybulletgym/envs/roboschool/scenes/../../assets/scenes/stadium/\")\n",
        "recursive_copy(f\"{libdir}/pybullet_data/\",f\"{libdir}/pybulletgym/envs/roboschool/robots/../../assets/\")\n",
        "\n",
        "#Step 3. Create 10x10 pixel empty images in order to avoid render error\n",
        "import numpy as np\n",
        "import numpngw \n",
        "img = np.zeros((10, 10, 3), dtype=np.uint8)\n",
        "img=img+255\n",
        "images=[img]\n",
        "numpngw.write_apng('anim_random_actions.png', images, delay=1)\n",
        "numpngw.write_apng('anim_trpo.png', images, delay=1)\n",
        "numpngw.write_apng('anim_PPO.png', images, delay=1)\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuBXcArKx4O1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRHGL_xAQq33"
      },
      "source": [
        "## Import all libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuyJxnVru9zQ"
      },
      "source": [
        "import jdc\n",
        "import gym\n",
        "import pybullet\n",
        "import pybulletgym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import collections   \n",
        "import IPython.display  \n",
        "\n",
        "# import os \n",
        "# import yaml  \n",
        "# import glob\n",
        "\n",
        "from datetime import datetime as dt\n",
        "from datetime import timedelta\n",
        "\n",
        "gym.logger.set_level(40) #avoid some warning\n",
        "\n",
        "# import time\n",
        "# from datetime import datetime\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "from pg_utils import get_device, show_plot\n",
        "from pg_utils import apply_update, flatten, flat_grad, get_flat_params, detach_dist\n",
        "from pg_utils import Transform, Bound, ZFilter\n",
        "\n",
        "print('done')\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN4b7Z9OYfZO"
      },
      "source": [
        "## Pybullet environment description. Run this cell in order to get familiar with an ant you will train.\n",
        "\n",
        "\n",
        "\n",
        "The Ant has 8 joints/actions and 28 observations. The reward function has a positive reward signal for the pelvic velocity, a negative reward signal for the effort of the current action state, and a negative signal if the joints are at their limit. The termination function triggers if the body of the ant rotates more than 0.2 units from upright.\n",
        "\n",
        "You may investigate source code in order to get more information https://github.com/openai/gym/blob/master/gym/envs/mujoco/ant.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8jpFf95bmkh"
      },
      "source": [
        "#Run this cell in order to get familiar with an environment.\n",
        "#No agent. Just random actions.\n",
        "\n",
        "  \n",
        "env_name = config_env_name  #see list of environments https://github.com/benelot/pybullet-gym/blob/master/README.md\n",
        "\n",
        "number_of_steps = 100\n",
        "\n",
        "env = gym.make(env_name)\n",
        "state = env.reset()\n",
        " \n",
        "images = []\n",
        "print(\"action_space=\")\n",
        "print(env.action_space.shape)    \n",
        "print(\"observation_space=\")\n",
        "print(env.observation_space.shape)    \n",
        "\n",
        "for _ in range(number_of_steps):\n",
        "    random_action = env.action_space.sample()\n",
        "    \n",
        "    state, reward, done, _ = env.step(random_action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    images.append(img)\n",
        "env.close()\n",
        "\n",
        " \n",
        "numpngw.write_apng('anim_random_actions.png', images, delay=1)\n",
        "IPython.display.Image(filename='anim_random_actions.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MZlLhrdLHE9"
      },
      "source": [
        "# \\-------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SkIQL9B_7VQ"
      },
      "source": [
        "# TPRO algorithm (Optional, no placeholders)\n",
        "\n",
        "Just investigate it and answer some questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeRfHsIGCA5C"
      },
      "source": [
        "## Math functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShAVuEq1M4Kx"
      },
      "source": [
        "### Conjecture gradient implementation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSiMv5apCAc2"
      },
      "source": [
        "def cg_solver(Avp_fun, b, max_iter=10):\n",
        "    '''\n",
        "    Finds an approximate solution to a set of linear equations Ax = b\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    Avp_fun : callable\n",
        "        a function that right multiplies a matrix A by a vector\n",
        "\n",
        "    b : torch.FloatTensor\n",
        "        the right hand term in the set of linear equations Ax = b\n",
        "\n",
        "    max_iter : int\n",
        "        the maximum number of iterations (default is 10)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    x : torch.FloatTensor\n",
        "        the approximate solution to the system of equations defined by Avp_fun\n",
        "        and b\n",
        "    '''\n",
        "\n",
        "    device = get_device()\n",
        "    x = torch.zeros_like(b).to(device)\n",
        "    r = b.clone()\n",
        "    p = b.clone()\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        Avp = Avp_fun(p, retain_graph=True)\n",
        "\n",
        "        alpha = torch.matmul(r, r) / torch.matmul(p, Avp)\n",
        "        x += alpha * p\n",
        "\n",
        "        if i == max_iter - 1:\n",
        "            return x\n",
        "\n",
        "        r_new = r - alpha * Avp\n",
        "        beta = torch.matmul(r_new, r_new) / torch.matmul(r, r)\n",
        "        r = r_new\n",
        "        p = r + beta * p\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcFVLtCtM-RP"
      },
      "source": [
        "### Kulbak-Leibner divergence (D_KL)\n",
        "$D_{KL}\\left ( P || Q \\right )=\\sum_{x\\in \\chi } P(x) log\\left (  \\frac{P(x)}{Q(x)} \\right )$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCGyaXv-M-Bt"
      },
      "source": [
        "def mean_kl_first_fixed(dist_1, dist_2):\n",
        "    '''\n",
        "    Calculate the kl-divergence between dist_1 and dist_2 after detaching dist_1\n",
        "    from the computational graph\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dist_1 : torch.distributions.distribution.Distribution\n",
        "        the first argument to the kl-divergence function (will be fixed)\n",
        "\n",
        "    dist_2 : torch.distributions.distribution.Distribution\n",
        "        the second argument to the kl-divergence function (will not be fixed)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    mean_kl : torch.float\n",
        "        the kl-divergence between dist_1 and dist_2\n",
        "    '''\n",
        "    dist_1_detached = detach_dist(dist_1)\n",
        "    mean_kl = torch.mean(torch.distributions.kl.kl_divergence(dist_1_detached, dist_2))\n",
        "\n",
        "    return mean_kl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyob_ioNKp4"
      },
      "source": [
        "### line_search()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWtcdBZkr8tx"
      },
      "source": [
        "def line_search(search_dir, max_step_len, constraints_satisfied, line_search_coef=0.9,\n",
        "                max_iter=10):\n",
        "    '''\n",
        "    Perform a backtracking line search that terminates when constraints_satisfied\n",
        "    return True and return the calculated step length. Return 0.0 if no step\n",
        "    length can be found for which constraints_satisfied returns True\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    search_dir : torch.FloatTensor\n",
        "        the search direction along which the line search is done\n",
        "\n",
        "    max_step_len : torch.FloatTensor\n",
        "        the maximum step length to consider in the line search\n",
        "\n",
        "    constraints_satisfied : callable\n",
        "        a function that returns a boolean indicating whether the constraints\n",
        "        are met by the current step length\n",
        "\n",
        "    line_search_coef : float\n",
        "        the proportion by which to reduce the step length after each iteration\n",
        "\n",
        "    max_iter : int\n",
        "        the maximum number of backtracks to do before return 0.0\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    the maximum step length coefficient for which constraints_satisfied evaluates\n",
        "    to True\n",
        "    '''\n",
        "\n",
        "    step_len = max_step_len / line_search_coef\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        step_len *= line_search_coef\n",
        "\n",
        "        if constraints_satisfied(step_len * search_dir, step_len):\n",
        "            return step_len\n",
        "\n",
        "    return torch.tensor(0.0).to(self.device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUbb8oRVI9RC"
      },
      "source": [
        "## Import TRPO class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8nU6FV-1aay"
      },
      "source": [
        "from trpo import Simulator, TRPO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4XPoChFaONV"
      },
      "source": [
        "## Add some methods to TRPO class "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MKfLE6uZynl"
      },
      "source": [
        "### Define train() method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw0hghIaZy8t"
      },
      "source": [
        "%%add_to TRPO\n",
        "def train(self, n_episodes):\n",
        "    \n",
        "    last_q = None\n",
        "    last_states = None\n",
        "    time_elapsed = False\n",
        "    reward_log=[]\n",
        "    while self.episode_num < n_episodes and not time_elapsed:\n",
        "        start_time = dt.now()\n",
        "        self.episode_num += 1\n",
        "        samples = self.simulator.sample_trajectories()\n",
        "        states, actions, rewards, q_vals = self.unroll_samples(samples)\n",
        "\n",
        "        advantages, states_with_time = self.get_advantages(samples)\n",
        "        advantages -= torch.mean(advantages)\n",
        "        advantages /= torch.std(advantages)\n",
        "\n",
        "        self.update_policy(states, actions, advantages)\n",
        "\n",
        "        if last_q is not None:\n",
        "            self.update_value_fun(torch.cat([states_with_time, last_states]), torch.cat([q_vals, last_q]))\n",
        "        else:\n",
        "            self.update_value_fun(states_with_time, q_vals)\n",
        "\n",
        "        last_q = q_vals\n",
        "        last_states = states_with_time\n",
        "\n",
        "        mean_reward = np.mean([np.sum(trajectory['rewards']) for trajectory in samples])\n",
        "        mean_reward_np = mean_reward\n",
        "        self.mean_rewards.append(mean_reward_np)\n",
        "        self.elapsed_time += dt.now() - start_time\n",
        "        self.print_update()\n",
        "        reward_log.append([self.episode_num,mean_reward])\n",
        "        show_plot(reward_log, config_pg_folder_path, \"TRPO\", 0.5)\n",
        "\n",
        "        if self.elapsed_time.seconds>config_train_time_minutes*60:\n",
        "            print('training time elapsed')\n",
        "            time_elapsed=True\n",
        "        \n",
        "\n",
        "        if self.save_every and not self.episode_num % self.save_every:\n",
        "            self.save_session(f\"{config_pg_folder_path}/{config_save_dir}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctmMJcBwWEEV"
      },
      "source": [
        "### Define get_advantages()  \n",
        "\n",
        "TASK:Define discounted residuals as multiplication of td_residual and  discount_pows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsLzthM7OLIu"
      },
      "source": [
        "%%add_to TRPO\n",
        "def get_advantages(self, samples):\n",
        "    advantages = []\n",
        "    states_with_time = []\n",
        "    T = self.simulator.trajectory_len\n",
        "\n",
        "    for trajectory in samples:\n",
        "        time = torch.arange(0, len(trajectory['rewards'])).unsqueeze(1).float() / T\n",
        "        states = torch.stack(trajectory['states'])\n",
        "        states = torch.cat([states, time], dim=-1)\n",
        "        states = states.to(self.device)\n",
        "        states_with_time.append(states.cpu())\n",
        "        rewards = torch.tensor(trajectory['rewards'])\n",
        "\n",
        "        state_values = self.value_fun(states)\n",
        "        state_values = state_values.view(-1)\n",
        "        state_values = state_values.cpu()\n",
        "        state_values_next = torch.cat([state_values[1:], torch.tensor([0.0])])\n",
        "\n",
        "        \n",
        "        td_residuals = rewards + self.discount * state_values_next - state_values  #PLACE HOLDER\n",
        "        \n",
        "        reverse = torch.arange(rewards.size(0) - 1, -1, -1)\n",
        "        \n",
        "        discount_pows = torch.pow(self.discount * self.lam, torch.arange(0, rewards.size(0)).float())\n",
        "        \n",
        "        discounted_residuals = td_residuals * discount_pows\n",
        "        \n",
        "        disc_res_sums = torch.cumsum(discounted_residuals[reverse], dim=-1)[reverse]\n",
        "\n",
        "\n",
        "\n",
        "        trajectory_advs = disc_res_sums / discount_pows\n",
        "        advantages.append(trajectory_advs)\n",
        "\n",
        "    advantages = torch.cat(advantages)\n",
        "\n",
        "    states_with_time = torch.cat(states_with_time)\n",
        "\n",
        "    return advantages, states_with_time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGuoq8cvWLCf"
      },
      "source": [
        "### Define update_value_fun() \n",
        "\n",
        "TASK: Add L2 regularization in value function. Use torch.sum(), torch.pow() or torch.square(). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FF3lzyeNfqE"
      },
      "source": [
        "%%add_to TRPO\n",
        "def update_value_fun(self, states, q_vals):\n",
        "    self.value_fun.train()\n",
        "\n",
        "    states = states.to(self.device)\n",
        "    q_vals = q_vals.to(self.device)\n",
        "\n",
        "    for i in range(self.vf_iters):\n",
        "        def mse():\n",
        "            self.value_optimizer.zero_grad()\n",
        "            state_values = self.value_fun(states).view(-1)\n",
        "\n",
        "            loss = self.mse_loss(state_values, q_vals)\n",
        "            flat_params=torch.cat([param.view(-1) for param in self.value_fun.parameters()])\n",
        "            \n",
        "            l2_loss = self.vf_l2_reg_coef * torch.sum(torch.pow(flat_params, 2))\n",
        "            loss += l2_loss\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            return loss\n",
        "\n",
        "        self.value_optimizer.step(mse)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cveBFhksqt8q"
      },
      "source": [
        "### Define Hessian closure function \n",
        "\n",
        "\n",
        "$H_f=\\begin{pmatrix} &\\frac{\\partial^2 }{\\partial x^2_1}  &\\frac{\\partial^2 }{\\partial x_1 \\partial x_2}    &... &\\frac{\\partial^2 }{\\partial x_1 \\partial x_n} \\\\  &\\frac{\\partial^2 }{\\partial x_2 \\partial x_1}  &\\frac{\\partial^2 }{\\partial x^2_2}  &... &\\frac{\\partial^2 }{\\partial x_2 \\partial x_n} \\\\  &\\vdots   &\\vdots   &\\ddots    &\\vdots \\\\  &\\frac{\\partial^2 }{\\partial x_n \\partial x_1}  &\\frac{\\partial^2 }{\\partial x_n \\partial x_2}  &... &\\frac{\\partial^2 }{\\partial x^2_n}\\end{pmatrix}$\n",
        "\n",
        "\n",
        "This function used in Conjecture gradient algorithm in order to find surrogate function maximum.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B318AXGNEhW"
      },
      "source": [
        "%%add_to TRPO\n",
        "def get_Hvp_fun(self,functional_output, inputs, damping_coef=0.0):\n",
        "    '''\n",
        "    Returns a function that calculates a Hessian-vector product with the Hessian\n",
        "    of functional_output w.r.t. inputs\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    functional_output : torch.FloatTensor (with requires_grad=True)\n",
        "        the output of the function of which the Hessian is calculated\n",
        "\n",
        "    inputs : torch.FloatTensor\n",
        "        the inputs w.r.t. which the Hessian is calculated\n",
        "\n",
        "    damping_coef : float\n",
        "        the multiple of the identity matrix to be added to the Hessian\n",
        "    '''\n",
        "\n",
        "    inputs = list(inputs)\n",
        "    \n",
        "    flat_grads = flat_grad(functional_output, inputs, create_graph=True)\n",
        "    #grads = torch.autograd.grad(loss, self.policy.parameters(), retain_graph=True, create_graph=True)\n",
        "    #flat_grads =  torch.cat([v.view(-1) for v in grads])\n",
        "\n",
        "    def Hvp_fun(v, retain_graph=True):\n",
        "        gvp = torch.matmul(flat_grads, v)\n",
        "        \n",
        "        Hvp = flat_grad(gvp, inputs, retain_graph=retain_graph)   #TODO ADD PLACEHOLDER\n",
        "\n",
        "        Hvp += damping_coef * v\n",
        "\n",
        "        return Hvp\n",
        "\n",
        "    return Hvp_fun"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFh01-yIJXZL"
      },
      "source": [
        "### Define update_policy() \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwrMmdrllv9a"
      },
      "source": [
        "%%add_to TRPO\n",
        "def update_policy(self, states, actions, advantages):\n",
        "    self.policy.train()\n",
        "\n",
        "    states = states.to(self.device)\n",
        "    actions = actions.to(self.device)\n",
        "    advantages = advantages.to(self.device)\n",
        "\n",
        "    action_dists = self.policy(states)\n",
        "    log_action_probs = action_dists.log_prob(actions)\n",
        "\n",
        "    loss = self.surrogate_loss(log_action_probs, log_action_probs.detach(), advantages)\n",
        "    \n",
        "    #loss_grad = flat_grad(loss, self.policy.parameters(), retain_graph=True)\n",
        "    grads = torch.autograd.grad(loss, self.policy.parameters(), retain_graph=True, create_graph=False)\n",
        "    flat_grads =  torch.cat([v.view(-1) for v in grads])\n",
        "    loss_grad=flat_grads\n",
        "\n",
        "    mean_kl = mean_kl_first_fixed(action_dists, action_dists)\n",
        "\n",
        "    Fvp_fun = self.get_Hvp_fun(mean_kl, self.policy.parameters())\n",
        "    search_dir = cg_solver(Fvp_fun, loss_grad, self.cg_max_iters)\n",
        "\n",
        "    expected_improvement = torch.matmul(loss_grad, search_dir)\n",
        "\n",
        "    def constraints_satisfied(step, beta):\n",
        "        apply_update(self.policy, step)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            new_action_dists = self.policy(states)\n",
        "            new_log_action_probs = new_action_dists.log_prob(actions)\n",
        "\n",
        "            new_loss = self.surrogate_loss(new_log_action_probs, log_action_probs, advantages)\n",
        "            mean_kl = mean_kl_first_fixed(action_dists, new_action_dists)\n",
        "             \n",
        "\n",
        "        actual_improvement = new_loss - loss  #### DELETE\n",
        "        improvement_ratio = actual_improvement / (expected_improvement * beta)  #DELETE\n",
        "\n",
        "        apply_update(self.policy, -step)\n",
        "\n",
        "        surrogate_cond = improvement_ratio >= self.line_search_accept_ratio and actual_improvement > 0.0\n",
        "        kl_cond = mean_kl <= self.max_kl_div\n",
        "\n",
        "        return surrogate_cond and kl_cond\n",
        "\n",
        "    max_step_len = self.get_max_step_len(search_dir, Fvp_fun, self.max_kl_div, retain_graph=True)\n",
        "    step_len = line_search(search_dir, max_step_len, constraints_satisfied)\n",
        "\n",
        "    opt_step = step_len * search_dir\n",
        "    apply_update(self.policy, opt_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmk_sqK1ZL38"
      },
      "source": [
        "###  Define surrogate loss  \n",
        "\n",
        "Implement the following formula for surrogate loss function:  \n",
        "\n",
        "\n",
        "$\\mathbb{E}_{a \\sim  q}\\left[ \\frac{\\pi_{\\theta}(a|s_n)}{q(a|s_n)} A_{\\theta_{old}}(s_n,a) \\right]$\n",
        "\n",
        "Take into account that program uses logprob so you need to use torch.exp() in order to get proper result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCJTvgSbXCmj"
      },
      "source": [
        "%%add_to TRPO\n",
        "def surrogate_loss(self, log_action_probs, imp_sample_probs, advantages):\n",
        "    return torch.mean(torch.exp(log_action_probs - imp_sample_probs) * advantages)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbUzyPaofVfK"
      },
      "source": [
        "## Train it!\n",
        "\n",
        "You may increace number of epoches to get more smart bot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edZPAJib3UN1"
      },
      "source": [
        "from layers import build_diag_gauss_policy, build_mlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY5kds0JLVpu"
      },
      "source": [
        "if config_use_trpo:\n",
        "\n",
        "    # Initialize the simulator\n",
        "    env_name = config_env_name #config['env_name']\n",
        "    n_episodes=1000 \n",
        "    n_trajectories = 50 \n",
        "    max_timesteps = 1000\n",
        "\n",
        "\n",
        "\n",
        "    continue_from_file = False\n",
        "    device = get_device()\n",
        "\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    action_space = env.action_space\n",
        "    observation_space = env.observation_space\n",
        "    policy_hidden_dims = [64, 64] \n",
        "    vf_hidden_dims = [64, 64]     \n",
        "    vf_args = (observation_space.shape[0] + 1, vf_hidden_dims, 1)\n",
        "\n",
        "    policy_args = (observation_space.shape[0], policy_hidden_dims, action_space.shape[0])\n",
        "    # build_diag_gauss_policy - implements a layer that outputs a Gaussian distribution with a diagonal\n",
        "    # covariance matrix\n",
        "    policy = build_diag_gauss_policy(*policy_args)\n",
        "\n",
        "\n",
        "    # Initalize the value function\n",
        "    # build_mlp - build a multilayer perceptron(mlp) with tanh activations with the specified input,\n",
        "    # output, and hidden layer sizes\n",
        "    value_fun = build_mlp(*vf_args)\n",
        "    policy.to(device)\n",
        "    value_fun.to(device)\n",
        "\n",
        "\n",
        "\n",
        "    env_args = {}\n",
        "\n",
        "\n",
        "    # Initialize the state transformation\n",
        "    # ZFilter - a z-scoring filter\n",
        "    z_filter = ZFilter()\n",
        "\n",
        "    # Bound - implements a bounding function\n",
        "    state_bound = Bound(-5, 5)\n",
        "\n",
        "    # Transform - composes several transformation and applies them sequentially\n",
        "    state_filter = Transform(state_bound, z_filter)\n",
        "\n",
        "\n",
        "    # It uses 50 gym environment objects at once.\n",
        "    simulator = Simulator(env_name, policy, n_trajectories,\n",
        "                          max_timesteps, state_filter=state_filter,\n",
        "                          **env_args)\n",
        "    trpo_args = {}\n",
        "    model_name='env1'\n",
        "    trpo = TRPO(policy, value_fun, simulator, model_name=model_name,\n",
        "                continue_from_file=continue_from_file, **trpo_args)\n",
        "\n",
        "    print(f'TRPO Starting training policy {model_name} on {env_name} environment...\\n')\n",
        "\n",
        "    trpo.train(n_episodes)\n",
        "\n",
        "    print('\\nTRPO Training complete.\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SYbhBc_a4Zw"
      },
      "source": [
        "## Render result. Run this cell in order to view policy in action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR4UrNn86Z4F"
      },
      "source": [
        "#render result\n",
        "if config_use_trpo:\n",
        "    max_timesteps = 1000\n",
        "    run_speed= 1\n",
        "\n",
        "    all_configs = {} \n",
        "    config = {} \n",
        "\n",
        "    device = get_device()\n",
        "\n",
        "    env_name = config_env_name\n",
        "    env = gym.make(env_name)\n",
        "    env._max_episode_steps = 1000\n",
        "\n",
        "    \n",
        "    action_space = env.action_space\n",
        "    observation_space = env.observation_space\n",
        "    policy_hidden_dims = [64,64] \n",
        "\n",
        "    policy_args = (observation_space.shape[0], policy_hidden_dims, action_space.shape[0])\n",
        "    policy = build_diag_gauss_policy(*policy_args)\n",
        "\n",
        "    session_dir = config_save_dir \n",
        "    load_path = os.path.join(config_pg_folder_path, session_dir, model_name + '.pt')\n",
        "\n",
        "    if device.type == 'cuda':\n",
        "        ckpt = torch.load(load_path)\n",
        "    else:\n",
        "        ckpt = torch.load(load_path, map_location='cpu')\n",
        "\n",
        "    policy.load_state_dict(ckpt['policy_state_dict'])\n",
        "    policy.to(device)\n",
        "    state_filter = ckpt['state_filter']\n",
        "\n",
        "    # Adjust the camera angle\n",
        "    # env.viewer.cam.lookat[0], env.viewer.cam.lookat[1], env.viewer.cam.lookat[2] = [0, -1.0, 1.15]\n",
        "    # env.viewer.cam.azimuth = 0\n",
        "    # env.viewer.cam.elevation = -30\n",
        "    # env.viewer.cam.fixedcamid = 0\n",
        "\n",
        "    # Run the simulation\n",
        "    policy.eval()\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    images = []\n",
        "    step=0\n",
        "\n",
        "    while not done:\n",
        "        state = torch.tensor(state).float()\n",
        "        state = state_filter(state).to(device)\n",
        "\n",
        "        action_dist = policy(state)\n",
        "        action = action_dist.sample().cpu()\n",
        "\n",
        "        state, reward, done, _ = env.step(action.numpy())\n",
        "        # env.render()\n",
        "        img = env.render(mode='rgb_array')\n",
        "        images.append(img)\n",
        "        step +=1\n",
        "    #    print(step)\n",
        "        if step>1000:\n",
        "            done=True\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    numpngw.write_apng(f\"{config_pg_folder_path}/anim_trpo.png\", images, delay=10)\n",
        "    IPython.display.Image(filename=f\"{config_pg_folder_path}/anim_trpo.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNeahJnnAH7T"
      },
      "source": [
        "IPython.display.Image(filename=f\"{config_pg_folder_path}/anim_trpo.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQBzpClAKyIQ"
      },
      "source": [
        "# \\-------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3hc4J3jbccw"
      },
      "source": [
        "# PPO algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7VPurFRbhEm"
      },
      "source": [
        "## Prepare PPO training 1. Define PPO helper classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl8P1iXmmHCR"
      },
      "source": [
        "### class RolloutBuffer\n",
        "\n",
        "Used to keep a trajectory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKbCzmkJmGwN"
      },
      "source": [
        "class RolloutBuffer:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "    \n",
        "\n",
        "    def clear(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.is_terminals[:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNxq6yozmNMm"
      },
      "source": [
        "### class ActorCritic "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEmE2c_-mM_6"
      },
      "source": [
        "\n",
        "class ActorCritic(torch.nn.Module):\n",
        "    def __init__(self, state_dim, action_dim,   action_std_init):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        \n",
        "        self.action_dim = action_dim\n",
        "        self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
        "\n",
        "        # actor\n",
        "        self.actor = nn.Sequential(\n",
        "                        nn.Linear(state_dim, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, action_dim),\n",
        "                        nn.Tanh()\n",
        "                    )\n",
        "      \n",
        "        \n",
        "        # critic\n",
        "        self.critic = nn.Sequential(\n",
        "                        nn.Linear(state_dim, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 1)\n",
        "                    )\n",
        "        \n",
        "    def set_action_std(self, new_action_std):\n",
        "        self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
        "        \n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQkA-A00cyGl"
      },
      "source": [
        "#### add act() method to ActorCritic class "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEWC5igGcx1t"
      },
      "source": [
        "%%add_to ActorCritic\n",
        "def act(self, state):\n",
        "\n",
        "    action_mean = self.actor(state)\n",
        "    cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
        "    dist = torch.distributions.MultivariateNormal(action_mean, cov_mat)\n",
        "    action = dist.sample()\n",
        "    action_logprob = dist.log_prob(action)\n",
        "    \n",
        "    return action.detach(), action_logprob.detach()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSGBUktKdOhl"
      },
      "source": [
        "#### add evaluate() method to ActorCritic class "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YjhWlzRdOP2"
      },
      "source": [
        "%%add_to ActorCritic\n",
        "def evaluate(self, state, action):\n",
        "    action_mean = self.actor(state)\n",
        "    action_var = self.action_var.expand_as(action_mean)\n",
        "    \n",
        "    cov_mat = torch.diag_embed(action_var).to(device)\n",
        "    dist = torch.distributions.MultivariateNormal(action_mean, cov_mat)\n",
        "      \n",
        "    # For Single Action Environments.\n",
        "    if self.action_dim == 1:\n",
        "        action = action.reshape(-1, self.action_dim)\n",
        "\n",
        "\n",
        "    action_logprobs = dist.log_prob(action)\n",
        "    dist_entropy = dist.entropy()\n",
        "    state_values = self.critic(state)\n",
        "      \n",
        "    return action_logprobs, state_values, dist_entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znon-HHQmXJf"
      },
      "source": [
        "## Prepare PPO training 2. Define PPO class "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ6CSLrmbewP"
      },
      "source": [
        "device = get_device()\n",
        " \n",
        "\n",
        "################################## PPO Policy ##################################\n",
        "\n",
        "\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip,  action_std_init=0.6):\n",
        "        self.action_std = action_std_init\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "        \n",
        "        self.buffer = RolloutBuffer()\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, action_std_init).to(device)\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
        "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
        "                    ])\n",
        "\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim,  action_std_init).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "        \n",
        "        self.MseLoss = torch.nn.MSELoss()\n",
        "\n",
        "\n",
        "    def set_action_std(self, new_action_std):\n",
        "        self.action_std = new_action_std\n",
        "        self.policy.set_action_std(new_action_std)\n",
        "        self.policy_old.set_action_std(new_action_std)\n",
        "\n",
        "\n",
        "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "        self.action_std = self.action_std - action_std_decay_rate\n",
        "        self.action_std = round(self.action_std, 4)\n",
        "        if (self.action_std <= min_action_std):\n",
        "            self.action_std = min_action_std\n",
        "            print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
        "        else:\n",
        "            print(\"setting actor output action_std to : \", self.action_std)\n",
        "        self.set_action_std(self.action_std)\n",
        "\n",
        "\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "    \n",
        "    def save(self, checkpoint_path):\n",
        "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
        "   \n",
        "\n",
        "    def load(self, checkpoint_path):\n",
        "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fl-ztwCfb_YY"
      },
      "source": [
        "### Define select_action() "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj7N7fJFbuU6"
      },
      "source": [
        "%%add_to PPO\n",
        "def select_action(self, state):\n",
        "    with torch.no_grad():\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "        action, action_logprob = self.policy_old.act(state)\n",
        "\n",
        "    self.buffer.states.append(state)\n",
        "    self.buffer.actions.append(action)\n",
        "    self.buffer.logprobs.append(action_logprob)\n",
        "\n",
        "    return action.detach().cpu().numpy().flatten()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgiOYD2AcCNR"
      },
      "source": [
        "### Define update_policy()[[*YOUR CODE HERE*]]\n",
        "\n",
        " \n",
        "\n",
        "$L^{CLIP}(\\theta)=\\hat{\\mathbb{E}}_t \\left[ min \\left(r_t(\\theta)\\hat{A}_t, clip \\left(r_t(\\theta),1-\\epsilon,1+\\epsilon \\right ) A_t \\right)\\right]$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pR__7Seb5Vc"
      },
      "source": [
        "%%add_to PPO\n",
        "def update_policy(self):\n",
        "\n",
        "    # Monte Carlo estimate of returns\n",
        "    rewards = []\n",
        "    discounted_reward = 0\n",
        "    for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
        "        if is_terminal:\n",
        "            discounted_reward = 0\n",
        "        discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "        rewards.insert(0, discounted_reward)\n",
        "        \n",
        "    # Normalizing the rewards\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "    rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
        "\n",
        "    # convert list to tensor\n",
        "    old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
        "    old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
        "    old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
        "\n",
        "    \n",
        "    # Optimize policy for K epochs\n",
        "    for _ in range(self.K_epochs):\n",
        "\n",
        "        # Evaluating old actions and values\n",
        "        logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "        # match state_values tensor dimensions with rewards tensor\n",
        "        state_values = torch.squeeze(state_values)\n",
        "        \n",
        "        # Finding the ratio (pi_theta / pi_theta__old)\n",
        "        \n",
        "        #  \n",
        "        ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "        # Finding Surrogate Loss\n",
        "        advantages = rewards - state_values.detach()\n",
        "\n",
        "        ## [[Your code here]]\n",
        "        ## Implement surrogate clip loss function.\n",
        "        ## Take epsion value from self.eps_clip\n",
        "        ## r=ratios\n",
        "        ## A=advantages\n",
        "        ## use torch.min() and torch.clamp() functions\n",
        "\n",
        "        L_CLIP = 0 #[[Your code here]]\n",
        "        loss = -L_CLIP + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
        "        \n",
        "        # take gradient step\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.mean().backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    # Copy new weights into old policy\n",
        "    self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "    # clear buffer\n",
        "    self.buffer.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8kXf5X6cVGx"
      },
      "source": [
        "## Train PPO!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zATgePs3mmIa"
      },
      "source": [
        "####### Initialize PPO environment hyperparameters ######\n",
        "\n",
        "\n",
        "env_name = config_env_name\n",
        "\n",
        "has_continuous_action_space = True  # continuous action space; else discrete\n",
        "\n",
        "max_ep_len = 500                   # max timesteps in one episode\n",
        "max_training_timesteps = int(3e6)   # break training loop if timeteps > max_training_timesteps\n",
        "\n",
        "print_freq = max_ep_len * 10        # print avg reward in the interval (in num timesteps)\n",
        "log_freq = max_ep_len * 2           # log avg reward in the interval (in num timesteps)\n",
        "save_model_freq = int(1e5)          # save model frequency (in num timesteps)\n",
        "\n",
        "action_std = 0.6                    # starting std for action distribution (Multivariate Normal)\n",
        "action_std_decay_rate = 0.05        # linearly decay action_std (action_std = action_std - action_std_decay_rate)\n",
        "min_action_std = 0.1                # minimum action_std (stop decay after action_std <= min_action_std)\n",
        "action_std_decay_freq = int(2.5e5)  # action_std decay frequency (in num timesteps)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "## Note : print/log frequencies should be > than max_ep_len\n",
        "\n",
        "\n",
        "################ PPO hyperparameters ################\n",
        "\n",
        "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
        "K_epochs = 80               # update policy for K epochs in one PPO update\n",
        "\n",
        "eps_clip = 0.2          # clip parameter for PPO\n",
        "gamma = 0.99            # discount factor\n",
        "\n",
        "lr_actor = 0.0003       # learning rate for actor network\n",
        "lr_critic = 0.001       # learning rate for critic network\n",
        "\n",
        "random_seed = 0         # set random seed if required (0 = no random seed)\n",
        "\n",
        "#####################################################\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG-GFfsucXM6"
      },
      "source": [
        "\n",
        "################################### Train PPO ###################################\n",
        "\n",
        "def train_PPO():\n",
        "\n",
        "    print(\"============================================================================================\")\n",
        "\n",
        "    print(\"training environment name : \" + env_name)\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    # state space dimension\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "\n",
        "    # action space dimension\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    \n",
        "\n",
        "\n",
        "    ###################### logging ######################\n",
        "\n",
        "    #### log files for multiple runs are NOT overwritten\n",
        "\n",
        "    log_dir = \"PPO_logs\"\n",
        "    if not os.path.exists(log_dir):\n",
        "          os.makedirs(log_dir)\n",
        "\n",
        "    log_dir = log_dir + '/' + env_name + '/'\n",
        "    if not os.path.exists(log_dir):\n",
        "          os.makedirs(log_dir)\n",
        "\n",
        "\n",
        "    #### get number of log files in log directory\n",
        "    run_num = 0\n",
        "    current_num_files = next(os.walk(log_dir))[2]\n",
        "    run_num = len(current_num_files)\n",
        "\n",
        "\n",
        "    #### create new log file for each run\n",
        "    log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "\n",
        "    print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
        "    print(\"logging at : \" + log_f_name)\n",
        "\n",
        "    #####################################################\n",
        "\n",
        "\n",
        "    ################### checkpointing ###################\n",
        "\n",
        "    run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
        "\n",
        "    directory = \"PPO_preTrained\"\n",
        "    if not os.path.exists(directory):\n",
        "          os.makedirs(directory)\n",
        "\n",
        "    directory = directory + '/' + env_name + '/'\n",
        "    if not os.path.exists(directory):\n",
        "          os.makedirs(directory)\n",
        "\n",
        "\n",
        "    checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "    print(\"save checkpoint path : \" + checkpoint_path)\n",
        "\n",
        "    #####################################################\n",
        "\n",
        "    \n",
        "\n",
        "    if random_seed:\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "        print(\"setting random seed to \", random_seed)\n",
        "        torch.manual_seed(random_seed)\n",
        "        env.seed(random_seed)\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "    #####################################################\n",
        "\n",
        "    print(\"============================================================================================\")\n",
        "\n",
        "    ################# training procedure ################\n",
        "\n",
        "    # initialize a PPO agent\n",
        "    ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip,  action_std)\n",
        "\n",
        "\n",
        "    # track total training time\n",
        "    start_time = dt.now().replace(microsecond=0)\n",
        "    print(\"Started training at (GMT) : \", start_time)\n",
        "\n",
        "    print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "    # logging file\n",
        "    log_f = open(log_f_name,\"w+\")\n",
        "    log_f.write('episode,timestep,reward\\n')\n",
        "\n",
        "\n",
        "    # printing and logging variables\n",
        "    print_running_reward = 0\n",
        "    print_running_episodes = 0\n",
        "\n",
        "    log_running_reward = 0\n",
        "    log_running_episodes = 0\n",
        "\n",
        "    time_step = 0\n",
        "    i_episode = 0\n",
        "    elapsed_time = timedelta(0)\n",
        "    start_time=dt.now()\n",
        "    time_elapsed = False\n",
        "    reward_log=[]\n",
        "    # training loop\n",
        "    while time_step <= max_training_timesteps and not time_elapsed:\n",
        "        elapsed_time = dt.now() - start_time\n",
        "        if  elapsed_time.seconds>config_train_time_minutes*60:\n",
        "            ppo_agent.save(f'{config_pg_folder_path}/PPO.pth')\n",
        "            print('training time elapsed')\n",
        "            time_elapsed=True\n",
        "\n",
        "        state = env.reset()\n",
        "        current_ep_reward = 0\n",
        "\n",
        "        for t in range(1, max_ep_len+1): #stops on done\n",
        "\n",
        "            # select action with policy\n",
        "            action = ppo_agent.select_action(state)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # saving reward and is_terminals\n",
        "            ppo_agent.buffer.rewards.append(reward)\n",
        "            ppo_agent.buffer.is_terminals.append(done)\n",
        "\n",
        "            time_step +=1\n",
        "            current_ep_reward += reward\n",
        "\n",
        "            # update PPO agent\n",
        "            if time_step % update_timestep == 0:\n",
        "                ppo_agent.update_policy()\n",
        "\n",
        "            # if continuous action space; then decay action std of ouput action distribution\n",
        "            if time_step % action_std_decay_freq == 0:\n",
        "                ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
        "\n",
        "            # log in logging file\n",
        "            if time_step % log_freq == 0:\n",
        "\n",
        "                # log average reward till last episode\n",
        "                log_avg_reward = log_running_reward / log_running_episodes\n",
        "                log_avg_reward = round(log_avg_reward, 4)\n",
        "\n",
        "                log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
        "                log_f.flush()\n",
        "\n",
        "                log_running_reward = 0\n",
        "                log_running_episodes = 0\n",
        "\n",
        "            # printing average reward\n",
        "            if time_step % print_freq == 0:\n",
        "\n",
        "                # print average reward till last episode\n",
        "                print_avg_reward = print_running_reward / print_running_episodes\n",
        "                print_avg_reward = round(print_avg_reward, 2)\n",
        "\n",
        "                print(\"PPO Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
        "\n",
        "                print_running_reward = 0\n",
        "                print_running_episodes = 0\n",
        "\n",
        "            # save model weights\n",
        "            if time_step % save_model_freq == 0:\n",
        "                print(\"--------------------------------------------------------------------------------------------\")\n",
        "                ppo_agent.save(f'{config_pg_folder_path}/PPO.pth')\n",
        "                print(\"Elapsed Time  : \", dt.now().replace(microsecond=0) - start_time)\n",
        "                print(\"--------------------------------------------------------------------------------------------\")\n",
        "                show_plot(reward_log, config_pg_folder_path, \"PPO\")\n",
        "\n",
        "\n",
        "            # break; if the episode is over\n",
        "            if done:\n",
        "                break\n",
        "        reward_log.append([i_episode,current_ep_reward])\n",
        "        print_running_reward += current_ep_reward\n",
        "        print_running_episodes += 1\n",
        "\n",
        "        log_running_reward += current_ep_reward\n",
        "        log_running_episodes += 1\n",
        "\n",
        "        i_episode += 1\n",
        "        if time_step % print_freq == 0 or time_step % save_model_freq == 0 or time_elapsed:\n",
        "            show_plot(reward_log, config_pg_folder_path, \"PPO\")\n",
        "\n",
        "\n",
        "    log_f.close()\n",
        "    env.close()\n",
        "\n",
        "\n",
        "    # print total training time\n",
        "    print(\"============================================================================================\")\n",
        "    end_time = dt.now().replace(microsecond=0)\n",
        "    print(\"Started training at (GMT) : \", start_time)\n",
        "    print(\"Finished training at (GMT) : \", end_time)\n",
        "    print(\"Total training time  : \", end_time - start_time)\n",
        "    print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "if config_use_ppo:\n",
        "    train_PPO()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiZ4IseDdPxI"
      },
      "source": [
        "## Render result. Run this cell in order to view PPO policy in action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RVgHDHhdSXE"
      },
      "source": [
        "def test_PPO():\n",
        "\n",
        "    print(\"============================================================================================\")\n",
        "\n",
        "    \n",
        "    env_name = config_env_name\n",
        "    has_continuous_action_space = True\n",
        "    max_ep_len = 300           # max timesteps in one episode\n",
        "    action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "    render = True              # render environment on screen\n",
        "    frame_delay = 0             # if required; add delay b/w frames\n",
        "\n",
        "\n",
        "    total_test_episodes = 1    # total num of testing episodes\n",
        "\n",
        "    K_epochs = 80               # update policy for K epochs\n",
        "    eps_clip = 0.2              # clip parameter for PPO\n",
        "    gamma = 0.99                # discount factor\n",
        "\n",
        "    lr_actor = 0.0003           # learning rate for actor\n",
        "    lr_critic = 0.001           # learning rate for critic\n",
        "\n",
        "    #####################################################\n",
        "\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    # state space dimension\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "\n",
        "    # action space dimension\n",
        "    action_dim = env.action_space.shape[0]\n",
        "\n",
        "    # initialize a PPO agent\n",
        "    ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip,  action_std)\n",
        "\n",
        "    random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
        "    run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
        "\n",
        "    print(\"loading network from PPO.pth \" )\n",
        "    ppo_agent.load(f\"{config_pg_folder_path}/PPO.pth\")\n",
        "\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "    test_running_reward = 0\n",
        "    images = []\n",
        "    for ep in range(1, total_test_episodes+1):\n",
        "        ep_reward = 0\n",
        "        state = env.reset()\n",
        "\n",
        "        for t in range(1, max_ep_len+1):\n",
        "            action = ppo_agent.select_action(state)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            ep_reward += reward\n",
        "            img = env.render(mode='rgb_array')\n",
        "            images.append(img)\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # clear buffer\n",
        "        ppo_agent.buffer.clear()\n",
        "\n",
        "        test_running_reward +=  ep_reward\n",
        "        print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
        "        ep_reward = 0\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "    print(\"============================================================================================\")\n",
        "\n",
        "    avg_test_reward = test_running_reward / total_test_episodes\n",
        "    avg_test_reward = round(avg_test_reward, 2)\n",
        "    print(\"average test reward : \" + str(avg_test_reward))\n",
        "\n",
        "    print(\"============================================================================================\")\n",
        "    \n",
        "   \n",
        "    numpngw.write_apng(f'{config_pg_folder_path}/anim_PPO.png', images, delay=10)\n",
        "\n",
        "\n",
        "\n",
        "if config_use_ppo:\n",
        "    test_PPO()\n",
        "    IPython.display.Image(filename=f'{config_pg_folder_path}/anim_PPO.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfe8p8yW_6RC"
      },
      "source": [
        "IPython.display.Image(filename=f'{config_pg_folder_path}/anim_PPO.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v35AlH3uarht"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}